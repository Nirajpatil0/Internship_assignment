{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from getpass import getpass\n",
        "from typing import List, Dict, Any, Optional\n",
        "import time\n",
        "import textwrap\n",
        "\n",
        "import openai"
      ],
      "metadata": {
        "id": "8aRWqGuO69Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Groq**"
      ],
      "metadata": {
        "id": "WB61qUODM3aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = getpass(\"my api key\")\n",
        "os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
        "\n",
        "# Create the client configured to talk to Groq's OpenAI-compatible endpoint\n",
        "\n",
        "client = openai.OpenAI(api_key=os.environ['GROQ_API_KEY'], base_url=\"https://api.groq.com/openai/v1\")\n",
        "print(\"Client configured. (Remember: do not commit your key to GitHub.)\")"
      ],
      "metadata": {
        "id": "yX-KKdH87MIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Small helper for calling chat completions (wraps responses)**"
      ],
      "metadata": {
        "id": "IFfLNp71Nl4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper wrapper to call the chat completions endpoint\n",
        "def call_chat_model(\n",
        "    messages: List[Dict[str, str]],\n",
        "    model: str = \"openai/gpt-oss-20b\",   # Groq-compatible model name\n",
        "    temperature: float = 0.2,\n",
        "    max_tokens: int = 512,\n",
        "    tools: Optional[List[Dict[str, Any]]] = None,\n",
        "    tool_choice: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calls the chat completions endpoint using the configured client and returns the raw response dict.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        kwargs = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens,\n",
        "        }\n",
        "        if tools is not None:\n",
        "            kwargs[\"tools\"] = tools\n",
        "\n",
        "\n",
        "        if tool_choice in [\"none\", \"auto\", \"required\"]:\n",
        "            kwargs[\"tool_choice\"] = tool_choice\n",
        "\n",
        "        resp = client.chat.completions.create(**kwargs)\n",
        "\n",
        "        # Convert to a regular dict\n",
        "        return json.loads(json.dumps(resp, default=lambda o: o.__dict__ if hasattr(o, \"__dict__\") else str(o)))\n",
        "    except Exception as e:\n",
        "        print(\"API call error:\", e)\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "ad74xEe393Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------\n",
        "**TASK 1: Conversation History Manager + Summarization**\n",
        "-------------------------------"
      ],
      "metadata": {
        "id": "ao6_Rg11Nvct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ConversationManager class**"
      ],
      "metadata": {
        "id": "__IOfLJXN9Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation manager - stores history, truncates, and summarizes using the model.\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = \"openai/gpt-oss-20b\",\n",
        "        max_turns: Optional[int] = None,    # if set, truncate to last N turns\n",
        "        max_chars: Optional[int] = None,    # if set, truncate by total characters\n",
        "        summarization_k: Optional[int] = None,  # run summarization after every k user messages\n",
        "        summary_threshold_turns: int = 20,  # if history grows above this, consider summarizing older parts\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.history: List[Dict[str,str]] = []\n",
        "        self.run_counter = 0\n",
        "        self.max_turns = max_turns\n",
        "        self.max_chars = max_chars\n",
        "        self.summarization_k = summarization_k\n",
        "        self.summary_threshold_turns = summary_threshold_turns\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        \"\"\"Add a message to history and trigger truncation/summarization logic.\"\"\"\n",
        "        assert role in (\"user\", \"assistant\", \"system\"), \"role must be one of 'user'|'assistant'|'system'\"\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        if role == \"user\":\n",
        "            self.run_counter += 1\n",
        "            if self.summarization_k and (self.run_counter % self.summarization_k == 0):\n",
        "                self.periodic_summarize()\n",
        "\n",
        "        # After every message insertion, apply truncation strategies if set\n",
        "        self.apply_truncation()\n",
        "\n",
        "    def apply_truncation(self):\n",
        "        \"\"\"Apply truncation by turns and/or characters if configured.\"\"\"\n",
        "\n",
        "        if self.max_turns and len(self.history) > self.max_turns:\n",
        "            removed = self.history[:-self.max_turns]\n",
        "            self.history = self.history[-self.max_turns:]\n",
        "            summary_text = self.quick_local_summary(removed)\n",
        "            self.history.insert(0, {\"role\": \"system\", \"content\": f\"Summary: {summary_text}\"})\n",
        "\n",
        "        # Truncate by characters (ensure total content length <= max_chars)\n",
        "        if self.max_chars:\n",
        "            total_len = sum(len(m[\"content\"]) for m in self.history)\n",
        "            if total_len > self.max_chars:\n",
        "                removed = []\n",
        "                while self.history and sum(len(m[\"content\"]) for m in self.history) > self.max_chars:\n",
        "                    removed.append(self.history.pop(0))\n",
        "                if removed:\n",
        "                    summary_text = self.quick_local_summary(removed)\n",
        "                    self.history.insert(0, {\"role\": \"system\", \"content\": f\"Summary: {summary_text}\"})\n",
        "\n",
        "    def quick_local_summary(self, messages: List[Dict[str,str]]) -> str:\n",
        "        \"\"\"\n",
        "        Quickly summarize messages locally (small heuristic) before calling model.\n",
        "        This provides a short context string for the model to further compress if needed.\n",
        "        \"\"\"\n",
        "        # Keep the first and last messages and count of messages by role for a quick summary\n",
        "        if not messages:\n",
        "            return \"\"\n",
        "        first = messages[0][\"content\"]\n",
        "        last = messages[-1][\"content\"]\n",
        "        counts = {}\n",
        "        for m in messages:\n",
        "            counts[m[\"role\"]] = counts.get(m[\"role\"], 0) + 1\n",
        "        return f\"Removed {len(messages)} messages (by roles {counts}). First: {first[:120]} ... Last: {last[:120]}\"\n",
        "\n",
        "    def periodic_summarize(self, summarization_prompt_prefix: str = None):\n",
        "        \"\"\"\n",
        "        Summarize older history using the LLM. We will summarize everything except the most recent messages.\n",
        "        The summarized text replaces the older messages.\n",
        "        \"\"\"\n",
        "        # Decide what to summarize: everything except last N messages (keep recent 6 messages)\n",
        "        keep_recent = 6\n",
        "        if len(self.history) <= keep_recent:\n",
        "            return  # nothing to summarize\n",
        "        to_summarize = self.history[:-keep_recent]\n",
        "        recent = self.history[-keep_recent:]\n",
        "\n",
        "        # Create a text blob to feed to the model\n",
        "        blob = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in to_summarize])\n",
        "        prompt_prefix = summarization_prompt_prefix or (\n",
        "            \"You are a concise summarizer. Summarize the following conversation history into short bullet points \"\n",
        "            \"that capture key user requests, decisions, and facts. Keep it short (<= 150 words).\"\n",
        "        )\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You summarize conversation history into concise bullet points.\"},\n",
        "            {\"role\": \"user\",   \"content\": prompt_prefix + \"\\n\\n\" + blob}\n",
        "        ]\n",
        "\n",
        "\n",
        "        resp = call_chat_model(messages=messages, model=self.model, temperature=0.0, max_tokens=250)\n",
        "        try:\n",
        "            summary_text = resp['choices'][0]['message']['content'].strip()\n",
        "        except Exception:\n",
        "            summary_text = \"Summary generated (fallback): \" + (blob[:200] + \"...\")\n",
        "\n",
        "        # Replace the older portion with a single system summary message, keep the recent\n",
        "        self.history = [{\"role\": \"system\", \"content\": f\"AutoSummary: {summary_text}\"}] + recent\n",
        "\n",
        "    def get_history(self) -> List[Dict[str,str]]:\n",
        "        \"\"\"Return a copy (for printing/demos).\"\"\"\n",
        "        return list(self.history)\n"
      ],
      "metadata": {
        "id": "59GPV_vX-zz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 demonstration: feed conversations and show outputs**"
      ],
      "metadata": {
        "id": "WYqG8-fYOrGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Demonstration for Task 1\n",
        "\n",
        "# Setup manager with specific truncation & periodic summarization settings\n",
        "mgr = ConversationManager(model=\"openai/gpt-oss-20b\", max_turns=12, max_chars=None, summarization_k=3)\n",
        "\n",
        "# Simulate a conversation flow (feed multiple user + assistant messages)\n",
        "samples = [\n",
        "    (\"user\", \"Hi, I'm working on a Groq assignment and need help building Chat summarization.\"),\n",
        "    (\"assistant\", \"Sure â€” what do you want to begin with?\"),\n",
        "    (\"user\", \"Explain how to maintain conversation history and summarize periodically.\"),\n",
        "    (\"assistant\", \"You can store messages in a list and call a summarization LLM periodically.\"),\n",
        "    (\"user\", \"Also show truncation by turns and character limits.\"),\n",
        "    (\"assistant\", \"Truncation by number of turns is straightforward: keep last N messages.\"),\n",
        "    (\"user\", \"Please summarize my earlier messages now.\"),  # this will be the 4th user message\n",
        "    (\"assistant\", \"Okay, I'll summarize when needed.\"),\n",
        "    (\"user\", \"Now add more messages to trigger the periodic summarization feature.\"),\n",
        "    (\"assistant\", \"I'll show how summarization happens after every 3rd user message.\"),\n",
        "    (\"user\", \"Message A\"),\n",
        "    (\"user\", \"Message B\"),\n",
        "    (\"user\", \"Message C\"),\n",
        "    (\"user\", \"Message D\"),\n",
        "    (\"assistant\", \"Now we've added several messages.\")\n",
        "]\n",
        "\n",
        "\n",
        "for role, content in samples:\n",
        "    mgr.add_message(role, content)\n",
        "\n",
        "# Print final history for inspection\n",
        "print(\"\\n===== Conversation history (after automatic truncation/summarization if triggered) =====\")\n",
        "for i, m in enumerate(mgr.get_history()):\n",
        "    print(f\"{i+1:02d} {m['role']}: {m['content'][:300]}\")  # show up to 300 chars per message\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yInfI1Dm_KLS",
        "outputId": "2425f72c-2fb4-47ed-ac38-418be14a039f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Conversation history (after automatic truncation/summarization if triggered) =====\n",
            "01 system: AutoSummary: \n",
            "02 user: Now add more messages to trigger the periodic summarization feature.\n",
            "03 assistant: I'll show how summarization happens after every 3rd user message.\n",
            "04 user: Message A\n",
            "05 user: Message B\n",
            "06 user: Message C\n",
            "07 user: Message D\n",
            "08 assistant: Now we've added several messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------\n",
        "**TASK 2: JSON Schema Classification & Extraction via Function Calling**\n",
        "-------------------------------"
      ],
      "metadata": {
        "id": "k6O3faOyO12T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the JSON schema as an OpenAI Function declaration**"
      ],
      "metadata": {
        "id": "-BZl6JMpO7Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function schema for extraction (OpenAI function-calling format)\n",
        "\n",
        "extract_function = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"extract_contact_info\",\n",
        "        \"description\": \"Extract contact and demographic fields (name, email, phone, location, age) from user text.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Full name of the person (if present).\"\n",
        "                },\n",
        "                \"email\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Email address. Return empty string if not present.\"\n",
        "                },\n",
        "                \"phone\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Phone number digits only or formatted. Return empty string if not present.\"\n",
        "                },\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"City, state or country the user mentioned (if any).\"\n",
        "                },\n",
        "                \"age\": {\n",
        "                    \"anyOf\": [\n",
        "                        {\"type\": \"integer\"},\n",
        "                        {\"type\": \"null\"}\n",
        "                    ],\n",
        "                    \"description\": \"Age as integer if mentioned; null if not mentioned.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": []  # allow partial extraction\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "0cfuTvKw_dhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper to call model with function calling and parse results**"
      ],
      "metadata": {
        "id": "jgA1WzliO95r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the model with function calling and parse the returned arguments\n",
        "\n",
        "def extract_with_function_call(user_text: str, model: str = \"openai/gpt-oss-20b\"):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a JSON extractor. Extract contact info into the requested schema.\"},\n",
        "        {\"role\": \"user\", \"content\": user_text}\n",
        "    ]\n",
        "    resp = call_chat_model(messages=messages, model=model, temperature=0.0, max_tokens=512,\n",
        "                       tools=[extract_function], tool_choice=\"auto\")\n",
        "\n",
        "    # Parse function_call arguments\n",
        "    try:\n",
        "        choice = resp['choices'][0]['message']\n",
        "        # If model chose to call the function: it will put a function_call object\n",
        "        if 'function_call' in choice and choice['function_call']:\n",
        "            arg_str = choice['function_call'].get('arguments', '{}')\n",
        "            extracted = json.loads(arg_str)\n",
        "        else:\n",
        "            content = choice.get('content', '')\n",
        "            try:\n",
        "                extracted = json.loads(content)\n",
        "            except Exception:\n",
        "                extracted = {}\n",
        "    except Exception as e:\n",
        "        print(\"Error parsing model response:\", e)\n",
        "        extracted = {}\n",
        "\n",
        "    return extracted, resp\n"
      ],
      "metadata": {
        "id": "dlpJzZBpmPiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation helpers for the extracted JSON**"
      ],
      "metadata": {
        "id": "YafyErYpPEIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraction + Validation helpers\n",
        "\n",
        "EMAIL_RE = re.compile(r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+\")\n",
        "\n",
        "def extract_with_function_call(user_text: str, model: str = \"gemma2-9b-it\"):\n",
        "    \"\"\"\n",
        "    Send text to model with tool schema and parse the structured JSON output.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Extract the contact details into structured JSON.\"},\n",
        "        {\"role\": \"user\", \"content\": user_text}\n",
        "    ]\n",
        "\n",
        "    resp = call_chat_model(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=0.0,\n",
        "        max_tokens=512,\n",
        "        tools=[extract_function],\n",
        "        tool_choice=\"auto\"\n",
        "    )\n",
        "\n",
        "    extracted = {}\n",
        "    try:\n",
        "        tool_calls = resp[\"choices\"][0][\"message\"].get(\"tool_calls\", [])\n",
        "        if tool_calls:\n",
        "            args_str = tool_calls[0][\"function\"][\"arguments\"]\n",
        "            extracted = json.loads(args_str)\n",
        "    except Exception as e:\n",
        "        print(\"Parsing error:\", e)\n",
        "\n",
        "    return extracted, resp\n",
        "\n",
        "\n",
        "def validate_extracted(data: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Validate fields in the extraction result. Return a dict with 'valid' True/False and issues found.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    validated = {}\n",
        "\n",
        "    # Name: accept if non-empty string\n",
        "    name = data.get('name') or \"\"\n",
        "    validated['name'] = name.strip() if isinstance(name, str) else \"\"\n",
        "    if not validated['name']:\n",
        "        issues.append(\"name_missing\")\n",
        "\n",
        "    # Email: basic regex\n",
        "    email = data.get('email') or \"\"\n",
        "    email = email.strip()\n",
        "    validated['email'] = email\n",
        "    if email and not EMAIL_RE.match(email):\n",
        "        issues.append(\"email_invalid\")\n",
        "\n",
        "    # Phone: strip non-digit chars; ensure something remains\n",
        "    phone_raw = data.get('phone') or \"\"\n",
        "    phone_digits = re.sub(r\"\\D\", \"\", phone_raw)\n",
        "    validated['phone'] = phone_digits\n",
        "    if phone_raw and not phone_digits:\n",
        "        issues.append(\"phone_invalid\")\n",
        "\n",
        "    # Location: accept string\n",
        "    location = data.get('location') or \"\"\n",
        "    validated['location'] = location.strip() if isinstance(location, str) else \"\"\n",
        "\n",
        "    # Age: ensure integer and sensible range (0 < age < 120)\n",
        "    age_raw = data.get('age', None)\n",
        "    age_val = None\n",
        "    if age_raw is not None and age_raw != \"\":\n",
        "        try:\n",
        "            age_val = int(age_raw)\n",
        "            if not (0 < age_val < 120):\n",
        "                issues.append(\"age_out_of_range\")\n",
        "        except Exception:\n",
        "            issues.append(\"age_not_integer\")\n",
        "            age_val = None\n",
        "    validated['age'] = age_val\n",
        "\n",
        "    return {\"validated\": validated, \"issues\": issues, \"is_valid\": len(issues) == 0}\n"
      ],
      "metadata": {
        "id": "Axxs92rJmSeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demonstration: parse 3 sample chats**"
      ],
      "metadata": {
        "id": "umbXXZnxPHgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Three sample chats demonstrating extraction\n",
        "\n",
        "samples = [\n",
        "    # 1) All info in one sentence\n",
        "    \"Hello, I'm Rahul Sharma from Pune. I'm 22 years old. My email is rahul.sharma@example.com and my phone is +91-98765-43210.\",\n",
        "    # 2) Scattered across sentences (no age provided)\n",
        "    \"Hi, can you save my contact? Name: Anjali Verma. Email: anjali_verma123@gmail.com. I'm currently based in Mumbai. Contact number 9876543210.\",\n",
        "    # 3) Partial / different formats (no email)\n",
        "    \"Hey, this is Sunil. Phone (US): (415) 555-0123. I live in San Francisco. Age twenty nine.\"\n",
        "]\n",
        "\n",
        "for i, s in enumerate(samples, 1):\n",
        "    print(\"\\n--- Sample\", i, \"---\")\n",
        "    print(\"User text:\", s)\n",
        "    extracted, raw_resp = extract_with_function_call(s)\n",
        "    print(\"Raw extracted JSON (model):\", json.dumps(extracted, indent=2))\n",
        "    validation = validate_extracted(extracted)\n",
        "    print(\"Validation result:\", validation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j42owcLCmWAf",
        "outputId": "f21f1555-096b-4da9-a2ae-d76827c7382b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "User text: Hello, I'm Rahul Sharma from Pune. I'm 22 years old. My email is rahul.sharma@example.com and my phone is +91-98765-43210.\n",
            "Raw extracted JSON (model): {\n",
            "  \"age\": 22,\n",
            "  \"email\": \"rahul.sharma@example.com\",\n",
            "  \"location\": \"Pune\",\n",
            "  \"name\": \"Rahul Sharma\",\n",
            "  \"phone\": \"+91-98765-43210\"\n",
            "}\n",
            "Validation result: {'validated': {'name': 'Rahul Sharma', 'email': 'rahul.sharma@example.com', 'phone': '919876543210', 'location': 'Pune', 'age': 22}, 'issues': [], 'is_valid': True}\n",
            "\n",
            "--- Sample 2 ---\n",
            "User text: Hi, can you save my contact? Name: Anjali Verma. Email: anjali_verma123@gmail.com. I'm currently based in Mumbai. Contact number 9876543210.\n",
            "Raw extracted JSON (model): {\n",
            "  \"age\": null,\n",
            "  \"email\": \"anjali_verma123@gmail.com\",\n",
            "  \"location\": \"Mumbai\",\n",
            "  \"name\": \"Anjali Verma\",\n",
            "  \"phone\": \"9876543210\"\n",
            "}\n",
            "Validation result: {'validated': {'name': 'Anjali Verma', 'email': 'anjali_verma123@gmail.com', 'phone': '9876543210', 'location': 'Mumbai', 'age': None}, 'issues': [], 'is_valid': True}\n",
            "\n",
            "--- Sample 3 ---\n",
            "User text: Hey, this is Sunil. Phone (US): (415) 555-0123. I live in San Francisco. Age twenty nine.\n",
            "Raw extracted JSON (model): {\n",
            "  \"age\": 29,\n",
            "  \"email\": \"\",\n",
            "  \"location\": \"San Francisco\",\n",
            "  \"name\": \"Sunil\",\n",
            "  \"phone\": \"(415) 555-0123\"\n",
            "}\n",
            "Validation result: {'validated': {'name': 'Sunil', 'email': '', 'phone': '4155550123', 'location': 'San Francisco', 'age': 29}, 'issues': [], 'is_valid': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DJgfWZhcmaW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}